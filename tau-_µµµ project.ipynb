{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Tau->µµµ machine learning basis** *by Louis Duval*\n\nThe contest and dataset is available here: https://www.kaggle.com/c/flavours-of-physics/overview/description\n\nkaggle notebook: https://www.kaggle.com/poultfloyd/tau-project/\n\nIn this project I choosed to work on BSM physics. These tau leptons to triple muons decays are said to be impossible in standard model. However it is interresting for rejection (or new physic!) to keep track of these kind of events.\nThis was originaly a contest made by CERN with more than 15 000€ cashprize and was supported by IFJ.\nI investigated some features of machine learning to try to see if some algorithm are more efficient or not.\n\n\nAs datasets are made with both MC simulations and real data, some verifications are necessary:\n* Some features can be attached more to MC rather than physics so we have to make sure we are learning on **physics** and not **MC artifacts**.\n* Each particle has its own mass. In an ideal world, one would just the mass of a particle to tell which particle it is. However, in reality, mass is an estimation, and it isn't a feature that scientists trust when building a model. Correlations with mass can cause an artificial signal-like mass peak or lead to incorrect background estimations. \n\nIn order to consider having a valid algorithm, the model has to pass some test (which I only adapted for my code, taken from here: https://www.kaggle.com/bdmj12/flavours-of-physics-3-competition-2 )."},{"metadata":{},"cell_type":"markdown","source":"We can start with some imports"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfiles_path=[]\nfiles_output=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        files_path.append(os.path.join(dirname, filename))\n\n\n        \n# Any results you write to the current directory are saved as output.\nimport csv\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n#from sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nimport zipfile\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here are some helpful functions from https://www.kaggle.com/bdmj12/flavours-of-physics-3-competition-2 to verify if the algorith passes the correlation and the agreement test"},{"metadata":{"trusted":true},"cell_type":"code","source":"### THIS CELL IS JUST THE EVALUATION PYTHON FILE \n\nimport numpy\nfrom sklearn.metrics import roc_curve, auc\n\n\ndef __rolling_window(data, window_size):\n    \"\"\"\n    Rolling window: take window with definite size through the array\n\n    :param data: array-like\n    :param window_size: size\n    :return: the sequence of windows\n\n    Example: data = array(1, 2, 3, 4, 5, 6), window_size = 4\n        Then this function return array(array(1, 2, 3, 4), array(2, 3, 4, 5), array(3, 4, 5, 6))\n    \"\"\"\n    shape = data.shape[:-1] + (data.shape[-1] - window_size + 1, window_size)\n    strides = data.strides + (data.strides[-1],)\n    return numpy.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n\n\ndef __cvm(subindices, total_events):\n    \"\"\"\n    Compute Cramer-von Mises metric.\n    Compared two distributions, where first is subset of second one.\n    Assuming that second is ordered by ascending\n\n    :param subindices: indices of events which will be associated with the first distribution\n    :param total_events: count of events in the second distribution\n    :return: cvm metric\n    \"\"\"\n    target_distribution = numpy.arange(1, total_events + 1, dtype='float') / total_events\n    subarray_distribution = numpy.cumsum(numpy.bincount(subindices, minlength=total_events), dtype='float')\n    subarray_distribution /= 1.0 * subarray_distribution[-1]\n    return numpy.mean((target_distribution - subarray_distribution) ** 2)\n\n\ndef compute_cvm(predictions, masses, n_neighbours=200, step=50):\n    \"\"\"\n    Computing Cramer-von Mises (cvm) metric on background events: take average of cvms calculated for each mass bin.\n    In each mass bin global prediction's cdf is compared to prediction's cdf in mass bin.\n\n    :param predictions: array-like, predictions\n    :param masses: array-like, in case of Kaggle tau23mu this is reconstructed mass\n    :param n_neighbours: count of neighbours for event to define mass bin\n    :param step: step through sorted mass-array to define next center of bin\n    :return: average cvm value\n    \"\"\"\n    predictions = numpy.array(predictions)\n    masses = numpy.array(masses)\n    assert len(predictions) == len(masses)\n\n    # First, reorder by masses\n    predictions = predictions[numpy.argsort(masses)]\n\n    # Second, replace probabilities with order of probability among other events\n    predictions = numpy.argsort(numpy.argsort(predictions, kind='mergesort'), kind='mergesort')\n\n    # Now, each window forms a group, and we can compute contribution of each group to CvM\n    cvms = []\n    for window in __rolling_window(predictions, window_size=n_neighbours)[::step]:\n        cvms.append(__cvm(subindices=window, total_events=len(predictions)))\n    return numpy.mean(cvms)\n\n\ndef __roc_curve_splitted(data_zero, data_one, sample_weights_zero, sample_weights_one):\n    \"\"\"\n    Compute roc curve\n\n    :param data_zero: 0-labeled data\n    :param data_one:  1-labeled data\n    :param sample_weights_zero: weights for 0-labeled data\n    :param sample_weights_one:  weights for 1-labeled data\n    :return: roc curve\n    \"\"\"\n    labels = [0] * len(data_zero) + [1] * len(data_one)\n    weights = numpy.concatenate([sample_weights_zero, sample_weights_one])\n    data_all = numpy.concatenate([data_zero, data_one])\n    fpr, tpr, _ = roc_curve(labels, data_all, sample_weight=weights)\n    return fpr, tpr\n\n\ndef compute_ks(data_prediction, mc_prediction, weights_data, weights_mc):\n    \"\"\"\n    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n\n    :param data_prediction: array-like, real data predictions\n    :param mc_prediction: array-like, Monte Carlo data predictions\n    :param weights_data: array-like, real data weights\n    :param weights_mc: array-like, Monte Carlo weights\n    :return: ks value\n    \"\"\"\n    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n\n    data_prediction, mc_prediction = numpy.array(data_prediction), numpy.array(mc_prediction)\n    weights_data, weights_mc = numpy.array(weights_data), numpy.array(weights_mc)\n\n    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n\n    weights_data /= numpy.sum(weights_data)\n    weights_mc /= numpy.sum(weights_mc)\n\n    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n\n    Dnm = numpy.max(numpy.abs(fpr - tpr))\n    return Dnm\n\ndef compute_ks_not_int(data_prediction, mc_prediction, weights_data, weights_mc):\n    \"\"\"\n    Compute Kolmogorov-Smirnov (ks) distance between real data predictions cdf and Monte Carlo one.\n\n    :param data_prediction: array-like, real data predictions\n    :param mc_prediction: array-like, Monte Carlo data predictions\n    :param weights_data: array-like, real data weights\n    :param weights_mc: array-like, Monte Carlo weights\n    :return: ks value\n    \"\"\"\n    assert len(data_prediction) == len(weights_data), 'Data length and weight one must be the same'\n    assert len(mc_prediction) == len(weights_mc), 'Data length and weight one must be the same'\n\n    data_prediction, mc_prediction = np.round(numpy.array(data_prediction)).astype(int), numpy.array(mc_prediction)\n    weights_data, weights_mc = numpy.array(weights_data), numpy.array(weights_mc)\n\n    for i in range(len(data_prediction)):\n        if data_prediction[i]<0:\n            data_prediction[i]=0\n        if data_prediction[i]>1:\n            data_prediction[i]=1\n            \n    for i in range(len(mc_prediction)):\n        if mc_prediction[i]<0:\n            mc_prediction[i]=0\n        if mc_prediction[i]>1:\n            mc_prediction[i]=1\n    \n    assert numpy.all(data_prediction >= 0.) and numpy.all(data_prediction <= 1.), 'Data predictions are out of range [0, 1]'\n    assert numpy.all(mc_prediction >= 0.) and numpy.all(mc_prediction <= 1.), 'MC predictions are out of range [0, 1]'\n\n    weights_data /= numpy.sum(weights_data)\n    weights_mc /= numpy.sum(weights_mc)\n\n    fpr, tpr = __roc_curve_splitted(data_prediction, mc_prediction, weights_data, weights_mc)\n\n    Dnm = numpy.max(numpy.abs(fpr - tpr))\n    return Dnm\n\n\n\ndef roc_auc_truncated(labels, predictions, tpr_thresholds=(0.2, 0.4, 0.6, 0.8),\n                      roc_weights=(4, 3, 2, 1, 0)):\n    \"\"\"\n    Compute weighted area under ROC curve.\n\n    :param labels: array-like, true labels\n    :param predictions: array-like, predictions\n    :param tpr_thresholds: array-like, true positive rate thresholds delimiting the ROC segments\n    :param roc_weights: array-like, weights for true positive rate segments\n    :return: weighted AUC\n    \"\"\"\n    assert numpy.all(predictions >= 0.) and numpy.all(predictions <= 1.), 'Data predictions are out of range [0, 1]'\n    assert len(tpr_thresholds) + 1 == len(roc_weights), 'Incompatible lengths of thresholds and weights'\n    fpr, tpr, _ = roc_curve(labels, predictions)\n    area = 0.\n    tpr_thresholds = [0.] + list(tpr_thresholds) + [1.]\n    for index in range(1, len(tpr_thresholds)):\n        tpr_cut = numpy.minimum(tpr, tpr_thresholds[index])\n        tpr_previous = numpy.minimum(tpr, tpr_thresholds[index - 1])\n        area += roc_weights[index - 1] * (auc(fpr, tpr_cut) - auc(fpr, tpr_previous))\n    tpr_thresholds = numpy.array(tpr_thresholds)\n    # roc auc normalization to be 1 for an ideal classifier\n    area /= numpy.sum((tpr_thresholds[1:] - tpr_thresholds[:-1]) * numpy.array(roc_weights))\n    return area\n\ndef check_corr_test(model,var):\n\n    check_correlation = pd.read_csv(folder + 'check_correlation.csv', index_col='id')\n    correlation_probs = model.predict_proba(check_correlation[var])[:, 1]\n    cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n    print('CvM metric', cvm, cvm < 0.002)\n    return cvm<0.002\n\ndef check_corr_test_get_value(model,var):\n\n    check_correlation = pd.read_csv(folder + 'check_correlation.csv', index_col='id')\n    correlation_probs = model.predict_proba(check_correlation[var])[:, 1]\n    cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n    #print('CvM metric', cvm, cvm < 0.002)\n    return cvm\n\n\ndef check_corr_test_GBR(model,var):\n\n    check_correlation = pd.read_csv(folder + 'check_correlation.csv', index_col='id')\n    correlation_probs = model.predict(check_correlation[var])\n    cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n    print('CvM metric', cvm, cvm < 0.002)\n    return cvm<0.002\n\n\ndef check_corr_test_get_value_GBR(model,var):\n\n    check_correlation = pd.read_csv(folder + 'check_correlation.csv', index_col='id')\n    correlation_probs = model.predict(check_correlation[var])\n    cvm = compute_cvm(correlation_probs, check_correlation['mass'])\n    #print('CvM metric', cvm, cvm < 0.002)\n    return cvm\n\ndef comp_auc(model,var):\n    train_eval = train[train['min_ANNmuon'] > 0.4]\n    train_probs = model.predict(train_eval[var])\n    AUC = roc_auc_truncated(train_eval['signal'], train_probs)\n    print('AUC', AUC)\n    return AUC\n\ndef check_ag_test(model,var):\n    check_agreement = pd.read_csv(folder + 'check_agreement.csv', index_col='id')\n    agreement_probs = model.predict(check_agreement[var])\n    \n    ks = compute_ks(\n        agreement_probs[check_agreement['signal'].values == 0],\n        agreement_probs[check_agreement['signal'].values == 1],\n        check_agreement[check_agreement['signal'] == 0]['weight'].values,\n        check_agreement[check_agreement['signal'] == 1]['weight'].values)\n    print('KS metric', ks, ks < 0.09)\n    return ks<0.09\n\ndef check_ag_test_get_value(model,var):\n    check_agreement = pd.read_csv(folder + 'check_agreement.csv', index_col='id')\n    agreement_probs = model.predict(check_agreement[var])\n    \n    ks = compute_ks_not_int(\n        agreement_probs[check_agreement['signal'].values == 0],\n        agreement_probs[check_agreement['signal'].values == 1],\n        check_agreement[check_agreement['signal'] == 0]['weight'].values,\n        check_agreement[check_agreement['signal'] == 1]['weight'].values)\n    #print('KS metric', ks, ks < 0.09)\n    return ks\n\ndef check_ag_test_not_int(model,var):\n    check_agreement = pd.read_csv(folder + 'check_agreement.csv', index_col='id')\n    agreement_probs = model.predict(check_agreement[var])\n    \n    ks = compute_ks_not_int(\n        agreement_probs[check_agreement['signal'].values == 0],\n        agreement_probs[check_agreement['signal'].values == 1],\n        check_agreement[check_agreement['signal'] == 0]['weight'].values,\n        check_agreement[check_agreement['signal'] == 1]['weight'].values)\n    print('KS metric', ks, ks < 0.09)\n    return ks<0.09\n\n\ndef pred_file(model,var):\n\n    test = pd.read_csv(folder + 'test.csv', index_col='id')\n    result = pd.DataFrame({'id': test.index})\n    result['prediction'] = model.predict(test[var])\n    result.to_csv('prediction %s .csv' % version, index=False, sep=',')\n    ## combine tests into one function\n\ndef eval(model,var):\n    check_ag_test(model,var)\n    check_corr_test(model,var)\n    comp_auc(model,var)\n    \ncandidate_models = {}   # we'll store candidate models here\n\ndef test_model(model):\n    #if the model passes the tests...\n    if(check_corr_test(model,variables) and check_ag_test(model,variables)):\n        #...add it to the candidates\n        candidate_models[svc] = comp_auc(model,variables)\n        print('passed')\n    else:\n        print('failed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start by extracting the files:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#extract zip files\nprint(files_path)\nfiles_out='/kaggle/working'\nfor files in files_path:\n    with zipfile.ZipFile(files, 'r') as zip_ref:\n        zip_ref.extractall(files_out)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now open them and have a closer look:"},{"metadata":{"trusted":true},"cell_type":"code","source":"files=[]\nfile=\"\"\nfolder=\"/kaggle/working/\"\n#extract filenames\nfor dirname, _, filenames in os.walk(folder):\n    files.append(filenames)\nfiles=files[0]\nprint(files)\n\n\n#store datas into a dictionnary\n#files can be used as an iterator to process everything\ncontest_data={}\nfor file in files:\n    contest_data[file]=pd.read_csv(folder + file, error_bad_lines=False)\nprint(contest_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contest_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset=contest_data['training.csv']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can investigate the main variables to spot differences between signal and not signal:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=5, ncols=9 ,figsize=(20,15), sharex=True)\nstart = 0\nmain_columns=[]\nfor i in training_dataset:\n    main_columns.append(i)\nfor j in range(5):\n    for i in range(9):\n        if start == len(main_columns):\n            break\n        sns.barplot(y=main_columns[start], x='signal', data=training_dataset, ax=ax[j,i])\n        ax[j,i].set_ylabel('')\n        ax[j,i].set_xlabel('')\n        ax[j,i].set_title(main_columns[start], fontsize=12)\n        start += 1\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See the correlation between signal and variables. Low correlation means low impact on determining if it is data or not, it can be usefull to remove variables with low correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=training_dataset.corr()['signal']\nplt.figure(figsize=(5,20))\nsns.heatmap(corr.to_frame().sort_values(by=\"signal\", ascending=False), annot=True, center=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=4)\nproj = pca.fit_transform(training_dataset)\nprint(proj.shape)\nplt.scatter(proj[:, 1], proj[:, 2], c=training_dataset['signal'], cmap=plt.get_cmap('Paired', 10))\nplt.plot()\nplt.colorbar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the datas are not clustered at all. It makes algorithm like k-neighbour not worth to test"},{"metadata":{},"cell_type":"markdown","source":"We remove data with low correlation or low physics meaning from variables and we define the training dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(abs(corr).to_frame().sort_values(by=\"signal\", ascending=True))\ntrain = pd.read_csv(\"/kaggle/working/training.csv\", index_col='id')\nvariables=train.drop([\"production\", \"min_ANNmuon\",\"signal\",\"mass\",\"SPDhits\",\"FlightDistanceError\"],axis=1).columns\ntraining_expected=contest_data['training.csv']['signal']\ntraining_data=contest_data['training.csv'][variables] \nagreement_expected=contest_data[\"check_agreement.csv\"]['signal']\nagreement_data=contest_data['check_agreement.csv'][variables]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First try with Gaussian Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ninit=np.random.randint(1,311000)\nconfusion_matrix=[[0,0],[0,0]]\nX_train, X_test, y_train, y_test = train_test_split(training_data, training_expected, random_state=init)\ngnb = GaussianNB()\nprint(gnb.fit(X_train, y_train))\n\n# use the model to predict the labels of the test data\npredicted = gnb.predict(X_test)\nexpected = y_test.to_numpy()\n\n#eval(gnb,variables)\nfor i in range(len(expected)):\n    # label the image with the target value\n    confusion_matrix[int(np.round(predicted[i]))][expected[i]]+=1\nprint(\"confusion matrix=\",confusion_matrix)\nprint(\"Identification ratio:\",(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[1][1]+confusion_matrix[1][0]+confusion_matrix[0][1]))\nif(check_corr_test(gnb,variables) and check_ag_test(gnb,variables)):\n    candidate_models[gnb] = comp_auc(gnb,variables)\n    print('passed')\nelse:\n    print(\"failed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score can be improved by dropping some variables, both Agreement and Correlation tests are passed"},{"metadata":{"trusted":true},"cell_type":"code","source":"init=np.random.randint(1,311000)\nclf=LinearDiscriminantAnalysis()\nX_train, X_test, y_train, y_test = train_test_split(training_data, training_expected, random_state=init)\nprint(clf.fit(X_train, y_train))\n\n# use the model to predict the labels of the test data\npredicted = clf.predict(X_test)\nexpected = y_test.to_numpy()\n\nprint(\"Score = \",clf.score(X_train, y_train))\n\n# split the data into training and validation sets\n#XP_train, XP_test, yP_train, yP_test = train_test_split(training_dataset, training_dataset['signal'], random_state=init)\n#confusion matrix\nconfusion_matrix=[[0,0],[0,0]]\nfor i in range(len(expected)):\n    # label the image with the target value\n    confusion_matrix[int(predicted[i])][expected[i]]+=1\nprint(confusion_matrix)\n\n\n\n\npredicted = clf.predict(X_test)\nexpected = y_test.to_numpy()\nconfusion_matrix=[[0,0],[0,0]]\nfor i in range(len(expected)):\n    # label the image with the target value\n    confusion_matrix[int(np.round(predicted[i]))][expected[i]]+=1\nprint(\"confusion matrix=\",confusion_matrix)\nprint(\"Identification ratio:\",(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[1][1]+confusion_matrix[1][0]+confusion_matrix[0][1]))\nif(check_corr_test(clf,variables) and check_ag_test(clf,variables)):\n    candidate_models[clf] = comp_auc(clf,variables)\n    print('passed')\nelse:\n    print(\"failed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both tests are once again passed. Better area under ROC curve tha GNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.1, 'loss': 'ls'}\ninit=np.random.randint(300000)\n#create data\nX_train, X_test, y_train, y_test = train_test_split(training_data, training_expected, random_state=init)\n\nscore_agreement=[]\n# create the model\nclf = ensemble.GradientBoostingRegressor(**params)\n\n#train data\nclf.fit(X_train, y_train)\n\n# use the model to predict the labels of the test data\npredicted = clf.predict(X_test)\nexpected=y_test.values\nagreement=y_train.values\nagreement_predicted=clf.predict(agreement_data)\n#score.append(clf.score(X_train, y_train))\nscore_agreement.append(clf.score(agreement_data,agreement_expected))\nprint(score_agreement)\nconfusion_matrix=[[0,0],[0,0]]\ntotal=0\nreussi=0\nfor i in range(0,len(predicted)):\n    total+=1\n    if int(np.round(predicted[i]))==expected[i]:\n        reussi+=1\n    # label the image with the target value\n    confusion_matrix[int(np.round(predicted[i]))][expected[i]]+=1\nprint(confusion_matrix)\nprint(total,reussi,reussi/total)\nif(check_corr_test_GBR(clf,variables) and check_ag_test_not_int(clf,variables)):\n    candidate_models[clf] = comp_auc(clf,variables)\n    \n    print('passed')\nelse:\n    print(\"failed\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correlation test is passed but agreement is bad. Issue with overtraining?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#overtraining proof:\nscore=[]\nscore_agreement=[]\nn_estimators=[]\nagreement_test=[]\ncorrelation_test=[]\n\nfor i in range(1,1001,100):\n\n    params = {'n_estimators': i, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.2, 'loss': 'ls'}\n\n# create the model\n    clf = ensemble.GradientBoostingRegressor(**params)\n\n    init=np.random.randint(300000)\n#create data\n    X_train, X_test, y_train, y_test = train_test_split(training_data, training_expected, random_state=init)\n\n#train data\n    clf.fit(X_train, y_train)\n    \n# use the model to predict the labels of the test data\n    predicted = clf.predict(X_test)\n    agreement_predicted=clf.predict(agreement_data)\n    score.append(clf.score(X_train, y_train))\n    score_agreement.append(clf.score(agreement_data,agreement_expected))\n    n_estimators.append(i)\n    agreement_test.append(check_ag_test_get_value(clf,variables))\n    correlation_test.append(check_corr_test_get_value_GBR(clf,variables))\n    \nprint(n_estimators, score,score_agreement)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(n_estimators, score)\nplt.ylabel('Score')\nplt.xlabel('Number of estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(n_estimators, score_agreement)\nplt.ylabel('Agreement score ')\nplt.xlabel('Number of estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(correlation_test,agreement_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(n_estimators, correlation_test)\nplt.ylabel('Correlation test score')\nplt.xlabel('Number of estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(n_estimators, agreement_test)\nplt.ylabel('Agreement test score')\nplt.xlabel('Number of estimators')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is hard to conclude on overtraining as the correlations are always too strong. "},{"metadata":{},"cell_type":"markdown","source":"These results have to be taken carefully: \"predict_proba\" makes GNB and LDA pass the correlation and the agreement test, but \"predict\" doesn't. As \"predict_proba\" doesn't exist for GBR, it can explain the fact that it does not pass the identification"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}